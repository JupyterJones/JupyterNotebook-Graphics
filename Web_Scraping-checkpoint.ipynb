{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scrape.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scrape.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\" This is a modified version of James Mills' original recipe. \"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import urllib2\n",
    "import urlparse\n",
    "import optparse\n",
    "import hashlib\n",
    "from cgi import escape\n",
    "from traceback import format_exc\n",
    "from Queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Link (object):\n",
    "\n",
    "    def __init__(self, src, dst, link_type):\n",
    "        self.src = src\n",
    "        self.dst = dst\n",
    "        self.link_type = link_type\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.src, self.dst, self.link_type))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.src == other.src and\n",
    "                self.dst == other.dst and\n",
    "                self.link_type == other.link_type)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.src + \" -> \" + self.dst\n",
    "\n",
    "class Crawler(object):\n",
    "\n",
    "    def __init__(self, root, depth_limit, confine=None, exclude=[], locked=True, filter_seen=True):\n",
    "        self.root = root\n",
    "        self.host = urlparse.urlparse(root)[1]\n",
    "\n",
    "        ## Data for filters:\n",
    "        self.depth_limit = depth_limit # Max depth (number of hops from root)\n",
    "        self.locked = locked           # Limit search to a single host?\n",
    "        self.confine_prefix=confine    # Limit search to this prefix\n",
    "        self.exclude_prefixes=exclude; # URL prefixes NOT to visit\n",
    "                \n",
    "\n",
    "        self.urls_seen = set()          # Used to avoid putting duplicates in queue\n",
    "        self.urls_remembered = set()    # For reporting to user\n",
    "        self.visited_links= set()       # Used to avoid re-processing a page\n",
    "        self.links_remembered = set()   # For reporting to user\n",
    "        \n",
    "        self.num_links = 0              # Links found (and not excluded by filters)\n",
    "        self.num_followed = 0           # Links followed.  \n",
    "\n",
    "        # Pre-visit filters:  Only visit a URL if it passes these tests\n",
    "        self.pre_visit_filters=[self._prefix_ok,\n",
    "                                self._exclude_ok,\n",
    "                                self._not_visited,\n",
    "                                self._same_host]\n",
    "\n",
    "        # Out-url filters: When examining a visited page, only process\n",
    "        # links where the target matches these filters.        \n",
    "        if filter_seen:\n",
    "            self.out_url_filters=[self._prefix_ok,\n",
    "                                     self._same_host]\n",
    "        else:\n",
    "            self.out_url_filters=[]\n",
    "\n",
    "    def _pre_visit_url_condense(self, url):\n",
    "        \n",
    "        \"\"\" Reduce (condense) URLs into some canonical form before\n",
    "        visiting.  All occurrences of equivalent URLs are treated as\n",
    "        identical.\n",
    "\n",
    "        All this does is strip the \\\"fragment\\\" component from URLs,\n",
    "        so that http://foo.com/blah.html\\#baz becomes\n",
    "        http://foo.com/blah.html \"\"\"\n",
    "\n",
    "        base, frag = urlparse.urldefrag(url)\n",
    "        return base\n",
    "\n",
    "    ## URL Filtering functions.  These all use information from the\n",
    "    ## state of the Crawler to evaluate whether a given URL should be\n",
    "    ## used in some context.  Return value of True indicates that the\n",
    "    ## URL should be used.\n",
    "    \n",
    "    def _prefix_ok(self, url):\n",
    "        \"\"\"Pass if the URL has the correct prefix, or none is specified\"\"\"\n",
    "        return (self.confine_prefix is None  or\n",
    "                url.startswith(self.confine_prefix))\n",
    "\n",
    "    def _exclude_ok(self, url):\n",
    "        \"\"\"Pass if the URL does not match any exclude patterns\"\"\"\n",
    "        prefixes_ok = [ not url.startswith(p) for p in self.exclude_prefixes]\n",
    "        return all(prefixes_ok)\n",
    "    \n",
    "    def _not_visited(self, url):\n",
    "        \"\"\"Pass if the URL has not already been visited\"\"\"\n",
    "        return (url not in self.visited_links)\n",
    "    \n",
    "    def _same_host(self, url):\n",
    "        \"\"\"Pass if the URL is on the same host as the root URL\"\"\"\n",
    "        try:\n",
    "            host = urlparse.urlparse(url)[1]\n",
    "            return re.match(\".*%s\" % self.host, host) \n",
    "        except Exception, e:\n",
    "            print >> sys.stderr, \"ERROR: Can't process url '%s' (%s)\" % (url, e)\n",
    "            return False\n",
    "            \n",
    "\n",
    "    def crawl(self):\n",
    "\n",
    "        \"\"\" Main function in the crawling process.  Core algorithm is:\n",
    "\n",
    "        q <- starting page\n",
    "        while q not empty:\n",
    "           url <- q.get()\n",
    "           if url is new and suitable:\n",
    "              page <- fetch(url)   \n",
    "              q.put(urls found in page)\n",
    "           else:\n",
    "              nothing\n",
    "\n",
    "        new and suitable means that we don't re-visit URLs we've seen\n",
    "        already fetched, and user-supplied criteria like maximum\n",
    "        search depth are checked. \"\"\"\n",
    "        \n",
    "        q = Queue()\n",
    "        q.put((self.root, 0))\n",
    "\n",
    "        while not q.empty():\n",
    "            this_url, depth = q.get()\n",
    "            \n",
    "            #Non-URL-specific filter: Discard anything over depth limit\n",
    "            if depth > self.depth_limit:\n",
    "                continue\n",
    "            \n",
    "            #Apply URL-based filters.\n",
    "            do_not_follow = [f for f in self.pre_visit_filters if not f(this_url)]\n",
    "            \n",
    "            #Special-case depth 0 (starting URL)\n",
    "            if depth == 0 and [] != do_not_follow:\n",
    "                print >> sys.stderr, \"Whoops! Starting URL %s rejected by the following filters:\", do_not_follow\n",
    "\n",
    "            #If no filters failed (that is, all passed), process URL\n",
    "            if [] == do_not_follow:\n",
    "                try:\n",
    "                    self.visited_links.add(this_url)\n",
    "                    self.num_followed += 1\n",
    "                    page = Fetcher(this_url)\n",
    "                    page.fetch()\n",
    "                    for link_url in [self._pre_visit_url_condense(l) for l in page.out_links()]:\n",
    "                        if link_url not in self.urls_seen:\n",
    "                            q.put((link_url, depth+1))\n",
    "                            self.urls_seen.add(link_url)\n",
    "                            \n",
    "                        do_not_remember = [f for f in self.out_url_filters if not f(link_url)]\n",
    "                        if [] == do_not_remember:\n",
    "                                self.num_links += 1\n",
    "                                self.urls_remembered.add(link_url)\n",
    "                                link = Link(this_url, link_url, \"href\")\n",
    "                                if link not in self.links_remembered:\n",
    "                                    self.links_remembered.add(link)\n",
    "                except Exception, e:\n",
    "                    print >>sys.stderr, \"ERROR: Can't process url '%s' (%s)\" % (this_url, e)\n",
    "                    #print format_exc()\n",
    "\n",
    "class OpaqueDataException (Exception):\n",
    "    def __init__(self, message, mimetype, url):\n",
    "        Exception.__init__(self, message)\n",
    "        self.mimetype=mimetype\n",
    "        self.url=url\n",
    "        \n",
    "\n",
    "class Fetcher(object):\n",
    "    \n",
    "    \"\"\"The name Fetcher is a slight misnomer: This class retrieves and interprets web pages.\"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.out_urls = []\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.out_urls[x]\n",
    "\n",
    "    def out_links(self):\n",
    "        return self.out_urls\n",
    "\n",
    "    #def _addHeaders(self, request):\n",
    "    #    request.add_header(\"User-Agent\", AGENT)\n",
    "\n",
    "    def _open(self):\n",
    "        url = self.url\n",
    "        try:\n",
    "            request = urllib2.Request(url)\n",
    "            handle = urllib2.build_opener()\n",
    "        except IOError:\n",
    "            return None\n",
    "        return (request, handle)\n",
    "\n",
    "    def fetch(self):\n",
    "        request, handle = self._open()\n",
    "        #self._addHeaders(request)\n",
    "        if handle:\n",
    "            try:\n",
    "                data=handle.open(request)\n",
    "                mime_type=data.info().gettype()\n",
    "                url=data.geturl();\n",
    "                if mime_type != \"text/html\":\n",
    "                    raise OpaqueDataException(\"Not interested in files of type %s\" % mime_type,\n",
    "                                              mime_type, url)\n",
    "                content = unicode(data.read(), \"utf-8\",\n",
    "                        errors=\"replace\")\n",
    "                soup = BeautifulSoup(content)\n",
    "                tags = soup('a')\n",
    "            except urllib2.HTTPError, error:\n",
    "                if error.code == 404:\n",
    "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
    "                else:\n",
    "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            except urllib2.URLError, error:\n",
    "                print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            except OpaqueDataException, error:\n",
    "                print >>sys.stderr, \"Skipping %s, has type %s\" % (error.url, error.mimetype)\n",
    "                tags = []\n",
    "            for tag in tags:\n",
    "                href = tag.get(\"href\")\n",
    "                if href is not None:\n",
    "                    url = urlparse.urljoin(self.url, escape(href))\n",
    "                    if url not in self:\n",
    "                        self.out_urls.append(url)\n",
    "\n",
    "def getLinks(url):\n",
    "    page = Fetcher(url)\n",
    "    page.fetch()\n",
    "    \"\"\"for i, url in enumerate(page):\n",
    "        print \"%d. %s\" % (i, url) \"\"\"\n",
    "    j = 1\n",
    "    for i, url in enumerate(page):\n",
    "        if url.find(\"http\")>=0:\n",
    "\t        print \"%d. %s\" % (j, url)\n",
    "\t        j = j + 1\n",
    "\n",
    "def parse_options():\n",
    "    \"\"\"parse_options() -> opts, args\n",
    "\n",
    "    Parse any command-line options given returning both\n",
    "    the parsed options and arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = optparse.OptionParser()\n",
    "\n",
    "    parser.add_option(\"-q\", \"--quiet\",\n",
    "            action=\"store_true\", default=False, dest=\"quiet\",\n",
    "            help=\"Enable quiet mode\")\n",
    "\n",
    "    parser.add_option(\"-l\", \"--links\",\n",
    "            action=\"store_true\", default=False, dest=\"links\",\n",
    "            help=\"Get links for specified url only\")    \n",
    "\n",
    "    parser.add_option(\"-d\", \"--depth\",\n",
    "            action=\"store\", type=\"int\", default=30, dest=\"depth_limit\",\n",
    "            help=\"Maximum depth to traverse\")\n",
    "\n",
    "    parser.add_option(\"-c\", \"--confine\",\n",
    "            action=\"store\", type=\"string\", dest=\"confine\",\n",
    "            help=\"Confine crawl to specified prefix\")\n",
    "\n",
    "    parser.add_option(\"-x\", \"--exclude\", action=\"append\", type=\"string\",\n",
    "                      dest=\"exclude\", default=[], help=\"Exclude URLs by prefix\")\n",
    "    \n",
    "    parser.add_option(\"-L\", \"--show-links\", action=\"store_true\", default=False,\n",
    "                      dest=\"out_links\", help=\"Output links found\")\n",
    "\n",
    "    parser.add_option(\"-u\", \"--show-urls\", action=\"store_true\", default=False,\n",
    "                      dest=\"out_urls\", help=\"Output URLs found\")\n",
    "\n",
    "    parser.add_option(\"-D\", \"--dot\", action=\"store_true\", default=False,\n",
    "                      dest=\"out_dot\", help=\"Output Graphviz dot file\")\n",
    "    \n",
    "\n",
    "\n",
    "    opts, args = parser.parse_args()\n",
    "\n",
    "    if len(args) < 1:\n",
    "        parser.print_help(sys.stderr)\n",
    "        raise SystemExit, 1\n",
    "\n",
    "    if opts.out_links and opts.out_urls:\n",
    "        parser.print_help(sys.stderr)\n",
    "        parser.error(\"options -L and -u are mutually exclusive\")\n",
    "\n",
    "    return opts, args\n",
    "\n",
    "class DotWriter:\n",
    "\n",
    "    \"\"\" Formats a collection of Link objects as a Graphviz (Dot)\n",
    "    graph.  Mostly, this means creating a node for each URL with a\n",
    "    name which Graphviz will accept, and declaring links between those\n",
    "    nodes.\"\"\"\n",
    "\n",
    "    def __init__ (self):\n",
    "        self.node_alias = {}\n",
    "\n",
    "    def _safe_alias(self, url, silent=False):\n",
    "\n",
    "        \"\"\"Translate URLs into unique strings guaranteed to be safe as\n",
    "        node names in the Graphviz language.  Currently, that's based\n",
    "        on the md5 digest, in hexadecimal.\"\"\"\n",
    "\n",
    "        if url in self.node_alias:\n",
    "            return self.node_alias[url]\n",
    "        else:\n",
    "            m = hashlib.md5()\n",
    "            m.update(url)\n",
    "            name = \"N\"+m.hexdigest()\n",
    "            self.node_alias[url]=name\n",
    "            if not silent:\n",
    "                print \"\\t%s [label=\\\"%s\\\"];\" % (name, url)                \n",
    "            return name\n",
    "\n",
    "\n",
    "    def asDot(self, links):\n",
    "\n",
    "        \"\"\" Render a collection of Link objects as a Dot graph\"\"\"\n",
    "        \n",
    "        print \"digraph Crawl {\"\n",
    "        print \"\\t edge [K=0.2, len=0.1];\"\n",
    "        for l in links:            \n",
    "            print \"\\t\" + self._safe_alias(l.src) + \" -> \" + self._safe_alias(l.dst) + \";\"\n",
    "        print  \"}\"\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "def main():    \n",
    "    opts, args = parse_options()\n",
    "\n",
    "    url = args[0]\n",
    "\n",
    "    if opts.links:\n",
    "        getLinks(url)\n",
    "        raise SystemExit, 0\n",
    "\n",
    "    depth_limit = opts.depth_limit\n",
    "    confine_prefix=opts.confine\n",
    "    exclude=opts.exclude\n",
    "\n",
    "    sTime = time.time()\n",
    "\n",
    "    print >> sys.stderr,  \"Crawling %s (Max Depth: %d)\" % (url, depth_limit)\n",
    "    crawler = Crawler(url, depth_limit, confine_prefix, exclude)\n",
    "    crawler.crawl()\n",
    "\n",
    "    if opts.out_urls:\n",
    "        print \"\\n\".join(crawler.urls_seen)\n",
    "\n",
    "    if opts.out_links:\n",
    "        print \"\\n\".join([str(l) for l in crawler.links_remembered])\n",
    "        \n",
    "    if opts.out_dot:\n",
    "        d = DotWriter()\n",
    "        d.asDot(crawler.links_remembered)\n",
    "\n",
    "    eTime = time.time()\n",
    "    tTime = eTime - sTime\n",
    "\n",
    "    print >> sys.stderr, \"Found:    %d\" % crawler.num_links\n",
    "    print >> sys.stderr, \"Followed: %d\" % crawler.num_followed\n",
    "    print >> sys.stderr, \"Stats:    (%d/s after %0.2fs)\" % (\n",
    "            int(math.ceil(float(crawler.num_links) / tTime)), tTime)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling https://jacknorthrup.com/Documentation/ (Max Depth: 30)\r\n",
      "ERROR: <urlopen error [Errno -2] Name or service not known>\r\n",
      "Found:    0\r\n",
      "Followed: 1\r\n",
      "Stats:    (0/s after 0.01s)\r\n"
     ]
    }
   ],
   "source": [
    "!python scrape.py https://jacknorthrup.com/Documentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\\n\\n<style>\\nbody {\\npadding:30px;\\nborder:1px solid gray;\\nbackground-color: #fff3e6;\\n}\\nh2{\\ntext-align:center;\\n}\\np{\\nfont-size:25px;\\ncolor:;\\t\\nmargin-top: 5px;\\nmargin-left: auto;\\nmargin-right: auto;\\nwidth:80%;\\nbackground-color: #fff3e6;\\n\\ntext-align:center;\\npadding:5px;\\nborder:1px solid gray;\\n}\\nhr{\\nwidth:70%;\\n}\\nul{\\nmargin-left:auto;\\nmargin-right:auto;\\ntext-align: center;\\n}\\ndiv.container {\\n    width: 100%;\\n    border: 1px solid gray;\\n    background-color: #e6ffff;\\n}\\n\\nheader, footer {\\n    padding: 1em;\\n    color: white;\\n    background-color: black;\\n    clear: left;\\n    text-align: center;\\n}\\n\\nnav {\\n    float: left;\\n    max-width: 160px;\\n    margin: 0;\\n    padding: 1em;\\n}\\n\\nnav ul {\\n    list-style-type: none;\\n    padding: 0;\\n}\\n   \\nnav ul a {\\n    text-decoration: none;\\n}\\n\\narticle {\\n    margin-left: 170px;\\n    border-left: 1px solid gray;\\n    padding: 1em;\\n    overflow: hidden;\\n}\\nli{\\ntext-decoration: underline;\\nmargin-bottom:10px;    \\n}\\n</style>\\n</head>\\n<body>\\n<div class =\"container\">\\n<header>\\n   <h1>Helper and Projects in Progress</h1>\\n</header>\\n<nav>\\n<ul>\\n<li><a href=\"http://jacknorthrup.com/link.html\">Jack Northrup LINKS</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks.org\">BLOGBLOCKS</a></li>\\n<li><a href=\"https://google.com\">GOOGLE LINK</a></li>\\n<li><a href=\"https://www.imagemagick.org/script/mogrify.php\">Mogrify</a></li>\\n<li><a href=\"http://jacknorthrup.com/FFMPEG.html\">FFMPEG</a></li>\\n<li><a href=\"https://www.w3schools.com/colors/colors_picker.asp\">COLOR PICKER</a></li>\\n<li><a href=\"https://drive.google.com/drive/my-drive\">GOOGLE DRIVE</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/parameters\">Some Fractal Parameters</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/28jupyternotebooktips.pdf\">Jupyter Notebook tips.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/AnacondaPython.html\">AnacondaPython.html</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/Bash.html\">Bash</a></li>\\n\\n<li><a href=\"http://jacknorthrup.com/Documentation/CAFFE_STUFF.html\">CAFFE_STUFF</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/conda-cheatsheet.pdf\">conda-cheatsheet.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DataBase.html\">DataBase</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DeepDreamStuff.html\">DeepDreamStuff</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DOCKER.html\">DOCKER</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/FFMPEG.html\">FFMPEG</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/IPython.html\">IPython</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/jupyter.pdf\">jupyter.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/misc_info.html\">misc_info</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/OpenCV.html\">OpenCV</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/OpenShift.html\">OpenShift</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/pil-handbook.pdf\">pil-handbook.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/PORTS.html\">PORTS</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/python.html\">Python</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/jscolor\">COLOR CODES</a></li>\\n</ul>\\n<br/>\\nSome of My Stuff On-line\\n<br/>\\n<ul>\\n<li><a href=\"https://jacknorthrup.wordpress.com/\">jacknorthrup.wordpress</a></li>\\n<li><a href=\"http://alicestamps.com/\"> Tweetyjill\\'s - alicestamps</a></li>\\n<li><a href=\"http://epistlerblogs.com/tutorials/\">epistlerblogs.com tutorials</a></li>\\n<li><a href=\"http://epistlerblogs.com/tutorials/Fish\">epistlerblogs.com tutorials/Fish</a></li>\\n<li><a href=\"http://epistlerblogs.com/AngularJS/\">epistlerblogs.com AngularJS</a></li>\\n<li><a href=\"http://epistlerblogs.com/chat/\">epistlerblogs.com chat/</a></li>\\n<li><a href=\"http://epistlerblogs.com/SpriteAnimation/\">epistlerblogs.com SpriteAnimation/</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com\">epistlerblogs.com freehandjack.com</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com/Smash\">freehandjack.com Smash</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com/Flash\">freehandjack.com Flash</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks\">epistlerblogs.com blogblocks</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/documentation/\">blogblocks documentation</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack/\">blogblocks  Jack</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">another blogblocks  Jack</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack/Jen-online/\">blogblocks Jack Jen-online</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/PH_world/\">blogblocks PH_world</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jenny/\"></a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/makeonline\">blogblocks Jenny</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/pdfpaper/\">blogblocks pdfpaper/</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Mini-Blogs/\">blogblocks Mini-Blogs</a></li>\\n<li><a href=\"http://www.blogblocks.net/Mini-Blogs/index.php\">Mini-Blogs</a></li>\\n<li><a href=\"http://www.blogblocks.net/life-in-the-philippines/\">life-in-the-philippines</a></li>\\n<li><a href=\"http://www.blogblocks.net/life-in-the-philippines-videos/\">Project philippines-videos</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/SALTMAN/\">SALTMAN</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Links/\">Paste Links</a></li>\\n<li><a href=\"http://www.myrareyes.com\">myrareyes</a></li>\\n<li><a href=\"http://www.vintagecollage.com/\">vintagecollage</a></li>\\n<li><a href=\"http://jacknorthrup.com\">jacknorthrup.com</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">another blogblocks  Jack</a></li>\\n</ul>\\n</nav>\\n<article>\\n<h2>PyInstaller Quickstart</h2>\\n<p>\\nInstall PyInstaller from PyPI:<br />\\n\\npip install pyinstaller<br />\\n\\nGo to your program\\xe2\\x80\\x99s directory and run:<br />\\n\\npyinstaller yourprogram.py<br />\\n\\nThis will generate the bundle in a subdirectory called dist.\\n</p><br /><br />\\n<hr>\\n<h2>Segment - runs from any where</h2>\\n<p>\\ncopied to: usr/local/bin<br />\\n\\n/home/jack/Desktop/dhoiem.cs/segment/\\n</p><br /><br />\\n<hr>\\n<h2>ZeroBraneStudio</h2> <br /> \\n\\n<p>\\n~/git/lua-5.3.4/ZeroBraneStudio$ <br /><br />\\n</p>\\n<hr>\\n\\n<h2>Creating  AN AppImage from android APP</h2>\\n\\n\\n\\n\\n<h2>opencv_test_photo</h2>\\n</article>\\n\\n\\n<footer>jack Northrup</footer>\\n</div>\\n\\n\\n\\n\\n\\n</body>\\n</html>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://127.0.0.1:8081\")\n",
    "page.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\\n  <style>\\n   body {\\npadding:30px;\\nborder:1px solid gray;\\nbackground-color: #fff3e6;\\n}\\nh2{\\ntext-align:center;\\n}\\np{\\nfont-size:25px;\\ncolor:;\\t\\nmargin-top: 5px;\\nmargin-left: auto;\\nmargin-right: auto;\\nwidth:80%;\\nbackground-color: #fff3e6;\\n\\ntext-align:center;\\npadding:5px;\\nborder:1px solid gray;\\n}\\nhr{\\nwidth:70%;\\n}\\nul{\\nmargin-left:auto;\\nmargin-right:auto;\\ntext-align: center;\\n}\\ndiv.container {\\n    width: 100%;\\n    border: 1px solid gray;\\n    background-color: #e6ffff;\\n}\\n\\nheader, footer {\\n    padding: 1em;\\n    color: white;\\n    background-color: black;\\n    clear: left;\\n    text-align: center;\\n}\\n\\nnav {\\n    float: left;\\n    max-width: 160px;\\n    margin: 0;\\n    padding: 1em;\\n}\\n\\nnav ul {\\n    list-style-type: none;\\n    padding: 0;\\n}\\n   \\nnav ul a {\\n    text-decoration: none;\\n}\\n\\narticle {\\n    margin-left: 170px;\\n    border-left: 1px solid gray;\\n    padding: 1em;\\n    overflow: hidden;\\n}\\nli{\\ntext-decoration: underline;\\nmargin-bottom:10px;    \\n}\\n  </style>\\n </head>\\n <body>\\n  <div class=\"container\">\\n   <header>\\n    <h1>\\n     Helper and Projects in Progress\\n    </h1>\\n   </header>\\n   <nav>\\n    <ul>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/link.html\">\\n       Jack Northrup LINKS\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks.org\">\\n       BLOGBLOCKS\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"https://google.com\">\\n       GOOGLE LINK\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"https://www.imagemagick.org/script/mogrify.php\">\\n       Mogrify\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/FFMPEG.html\">\\n       FFMPEG\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"https://www.w3schools.com/colors/colors_picker.asp\">\\n       COLOR PICKER\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"https://drive.google.com/drive/my-drive\">\\n       GOOGLE DRIVE\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/parameters\">\\n       Some Fractal Parameters\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/28jupyternotebooktips.pdf\">\\n       Jupyter Notebook tips.pdf\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/AnacondaPython.html\">\\n       AnacondaPython.html\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/Bash.html\">\\n       Bash\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/CAFFE_STUFF.html\">\\n       CAFFE_STUFF\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/conda-cheatsheet.pdf\">\\n       conda-cheatsheet.pdf\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/DataBase.html\">\\n       DataBase\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/DeepDreamStuff.html\">\\n       DeepDreamStuff\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/DOCKER.html\">\\n       DOCKER\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/FFMPEG.html\">\\n       FFMPEG\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/IPython.html\">\\n       IPython\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/jupyter.pdf\">\\n       jupyter.pdf\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/misc_info.html\">\\n       misc_info\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/OpenCV.html\">\\n       OpenCV\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/OpenShift.html\">\\n       OpenShift\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/pil-handbook.pdf\">\\n       pil-handbook.pdf\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/PORTS.html\">\\n       PORTS\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com/Documentation/python.html\">\\n       Python\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/jscolor\">\\n       COLOR CODES\\n      </a>\\n     </li>\\n    </ul>\\n    <br/>\\n    Some of My Stuff On-line\\n    <br/>\\n    <ul>\\n     <li>\\n      <a href=\"https://jacknorthrup.wordpress.com/\">\\n       jacknorthrup.wordpress\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://alicestamps.com/\">\\n       Tweetyjill\\'s - alicestamps\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/tutorials/\">\\n       epistlerblogs.com tutorials\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/tutorials/Fish\">\\n       epistlerblogs.com tutorials/Fish\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/AngularJS/\">\\n       epistlerblogs.com AngularJS\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/chat/\">\\n       epistlerblogs.com chat/\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/SpriteAnimation/\">\\n       epistlerblogs.com SpriteAnimation/\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/freehandjack.com\">\\n       epistlerblogs.com freehandjack.com\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/freehandjack.com/Smash\">\\n       freehandjack.com Smash\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/freehandjack.com/Flash\">\\n       freehandjack.com Flash\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks\">\\n       epistlerblogs.com blogblocks\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/documentation/\">\\n       blogblocks documentation\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Jack/\">\\n       blogblocks  Jack\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">\\n       another blogblocks  Jack\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Jack/Jen-online/\">\\n       blogblocks Jack Jen-online\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/PH_world/\">\\n       blogblocks PH_world\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Jenny/\">\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/makeonline\">\\n       blogblocks Jenny\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/pdfpaper/\">\\n       blogblocks pdfpaper/\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Mini-Blogs/\">\\n       blogblocks Mini-Blogs\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://www.blogblocks.net/Mini-Blogs/index.php\">\\n       Mini-Blogs\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://www.blogblocks.net/life-in-the-philippines/\">\\n       life-in-the-philippines\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://www.blogblocks.net/life-in-the-philippines-videos/\">\\n       Project philippines-videos\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/SALTMAN/\">\\n       SALTMAN\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Links/\">\\n       Paste Links\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://www.myrareyes.com\">\\n       myrareyes\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://www.vintagecollage.com/\">\\n       vintagecollage\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://jacknorthrup.com\">\\n       jacknorthrup.com\\n      </a>\\n     </li>\\n     <li>\\n      <a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">\\n       another blogblocks  Jack\\n      </a>\\n     </li>\\n    </ul>\\n   </nav>\\n   <article>\\n    <h2>\\n     PyInstaller Quickstart\\n    </h2>\\n    <p>\\n     Install PyInstaller from PyPI:\\n     <br/>\\n     pip install pyinstaller\\n     <br/>\\n     Go to your program\\u2019s directory and run:\\n     <br/>\\n     pyinstaller yourprogram.py\\n     <br/>\\n     This will generate the bundle in a subdirectory called dist.\\n    </p>\\n    <br/>\\n    <br/>\\n    <hr>\\n     <h2>\\n      Segment - runs from any where\\n     </h2>\\n     <p>\\n      copied to: usr/local/bin\\n      <br/>\\n      /home/jack/Desktop/dhoiem.cs/segment/\\n     </p>\\n     <br/>\\n     <br/>\\n     <hr>\\n      <h2>\\n       ZeroBraneStudio\\n      </h2>\\n      <br/>\\n      <p>\\n       ~/git/lua-5.3.4/ZeroBraneStudio$\\n       <br/>\\n       <br/>\\n      </p>\\n      <hr>\\n       <h2>\\n        Creating  AN AppImage from android APP\\n       </h2>\\n       <h2>\\n        opencv_test_photo\\n       </h2>\\n      </hr>\\n     </hr>\\n    </hr>\\n   </article>\\n   <footer>\\n    jack Northrup\\n   </footer>\\n  </div>\\n </body>\\n</html>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://127.0.0.1:8081\")\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "htmnew = soup.prettify()\n",
    "htmnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'html',\n",
       " u'\\n',\n",
       " <html lang=\"en\">\\n<head>\\n<meta content=\"text/html; charset=unicode-escape\" http-equiv=\"Content-Type\"/>\\n<style>\\nbody {\\npadding:30px;\\nborder:1px solid gray;\\nbackground-color: #fff3e6;\\n}\\nh2{\\ntext-align:center;\\n}\\np{\\nfont-size:25px;\\ncolor:;\\t\\nmargin-top: 5px;\\nmargin-left: auto;\\nmargin-right: auto;\\nwidth:80%;\\nbackground-color: #fff3e6;\\n\\ntext-align:center;\\npadding:5px;\\nborder:1px solid gray;\\n}\\nhr{\\nwidth:70%;\\n}\\nul{\\nmargin-left:auto;\\nmargin-right:auto;\\ntext-align: center;\\n}\\ndiv.container {\\n    width: 100%;\\n    border: 1px solid gray;\\n    background-color: #e6ffff;\\n}\\n\\nheader, footer {\\n    padding: 1em;\\n    color: white;\\n    background-color: black;\\n    clear: left;\\n    text-align: center;\\n}\\n\\nnav {\\n    float: left;\\n    max-width: 160px;\\n    margin: 0;\\n    padding: 1em;\\n}\\n\\nnav ul {\\n    list-style-type: none;\\n    padding: 0;\\n}\\n   \\nnav ul a {\\n    text-decoration: none;\\n}\\n\\narticle {\\n    margin-left: 170px;\\n    border-left: 1px solid gray;\\n    padding: 1em;\\n    overflow: hidden;\\n}\\nli{\\ntext-decoration: underline;\\nmargin-bottom:10px;    \\n}\\n</style>\\n</head>\\n<body>\\n<div class=\"container\">\\n<header>\\n<h1>Helper and Projects in Progress</h1>\\n</header>\\n<nav>\\n<ul>\\n<li><a href=\"http://jacknorthrup.com/link.html\">Jack Northrup LINKS</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks.org\">BLOGBLOCKS</a></li>\\n<li><a href=\"https://google.com\">GOOGLE LINK</a></li>\\n<li><a href=\"https://www.imagemagick.org/script/mogrify.php\">Mogrify</a></li>\\n<li><a href=\"http://jacknorthrup.com/FFMPEG.html\">FFMPEG</a></li>\\n<li><a href=\"https://www.w3schools.com/colors/colors_picker.asp\">COLOR PICKER</a></li>\\n<li><a href=\"https://drive.google.com/drive/my-drive\">GOOGLE DRIVE</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/parameters\">Some Fractal Parameters</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/28jupyternotebooktips.pdf\">Jupyter Notebook tips.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/AnacondaPython.html\">AnacondaPython.html</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/Bash.html\">Bash</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/CAFFE_STUFF.html\">CAFFE_STUFF</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/conda-cheatsheet.pdf\">conda-cheatsheet.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DataBase.html\">DataBase</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DeepDreamStuff.html\">DeepDreamStuff</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/DOCKER.html\">DOCKER</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/FFMPEG.html\">FFMPEG</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/IPython.html\">IPython</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/jupyter.pdf\">jupyter.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/misc_info.html\">misc_info</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/OpenCV.html\">OpenCV</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/OpenShift.html\">OpenShift</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/pil-handbook.pdf\">pil-handbook.pdf</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/PORTS.html\">PORTS</a></li>\\n<li><a href=\"http://jacknorthrup.com/Documentation/python.html\">Python</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/jscolor\">COLOR CODES</a></li>\\n</ul>\\n<br/>\\nSome of My Stuff On-line\\n<br/>\\n<ul>\\n<li><a href=\"https://jacknorthrup.wordpress.com/\">jacknorthrup.wordpress</a></li>\\n<li><a href=\"http://alicestamps.com/\"> Tweetyjill's - alicestamps</a></li>\\n<li><a href=\"http://epistlerblogs.com/tutorials/\">epistlerblogs.com tutorials</a></li>\\n<li><a href=\"http://epistlerblogs.com/tutorials/Fish\">epistlerblogs.com tutorials/Fish</a></li>\\n<li><a href=\"http://epistlerblogs.com/AngularJS/\">epistlerblogs.com AngularJS</a></li>\\n<li><a href=\"http://epistlerblogs.com/chat/\">epistlerblogs.com chat/</a></li>\\n<li><a href=\"http://epistlerblogs.com/SpriteAnimation/\">epistlerblogs.com SpriteAnimation/</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com\">epistlerblogs.com freehandjack.com</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com/Smash\">freehandjack.com Smash</a></li>\\n<li><a href=\"http://epistlerblogs.com/freehandjack.com/Flash\">freehandjack.com Flash</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks\">epistlerblogs.com blogblocks</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/documentation/\">blogblocks documentation</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack/\">blogblocks  Jack</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">another blogblocks  Jack</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack/Jen-online/\">blogblocks Jack Jen-online</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/PH_world/\">blogblocks PH_world</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jenny/\"></a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/makeonline\">blogblocks Jenny</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/pdfpaper/\">blogblocks pdfpaper/</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Mini-Blogs/\">blogblocks Mini-Blogs</a></li>\\n<li><a href=\"http://www.blogblocks.net/Mini-Blogs/index.php\">Mini-Blogs</a></li>\\n<li><a href=\"http://www.blogblocks.net/life-in-the-philippines/\">life-in-the-philippines</a></li>\\n<li><a href=\"http://www.blogblocks.net/life-in-the-philippines-videos/\">Project philippines-videos</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/SALTMAN/\">SALTMAN</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Links/\">Paste Links</a></li>\\n<li><a href=\"http://www.myrareyes.com\">myrareyes</a></li>\\n<li><a href=\"http://www.vintagecollage.com/\">vintagecollage</a></li>\\n<li><a href=\"http://jacknorthrup.com\">jacknorthrup.com</a></li>\\n<li><a href=\"http://epistlerblogs.com/blogblocks/Jack2/\">another blogblocks  Jack</a></li>\\n</ul>\\n</nav>\\n<article>\\n<h2>PyInstaller Quickstart</h2>\\n<p>\\nInstall PyInstaller from PyPI:<br/>\\n\\npip install pyinstaller<br/>\\n\\nGo to your program\\u2019s directory and run:<br/>\\n\\npyinstaller yourprogram.py<br/>\\n\\nThis will generate the bundle in a subdirectory called dist.\\n</p><br/><br/>\\n<hr>\\n<h2>Segment - runs from any where</h2>\\n<p>\\ncopied to: usr/local/bin<br/>\\n\\n/home/jack/Desktop/dhoiem.cs/segment/\\n</p><br/><br/>\\n<hr>\\n<h2>ZeroBraneStudio</h2> <br/>\\n<p>\\n~/git/lua-5.3.4/ZeroBraneStudio$ <br/><br/>\\n</p>\\n<hr>\\n<h2>Creating  AN AppImage from android APP</h2>\\n<h2>opencv_test_photo</h2>\\n</hr></hr></hr></article>\\n<footer>jack Northrup</footer>\\n</div>\\n</body>\\n</html>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(soup.children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-079451b850c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named Request"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from urllib2.Request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.newegg.com/global/ph/Processors-Desktops/SubCategory/ID-343?nm_mc=KNC-GooglephAdwords&cm_mmc=KNC-GooglephAdwords-_-Sitelink-Philippines-_-CPUs-_-Global&gclid=Cj0KCQjw_JrMBRDPARIsACis1HyfF4COWw3RstUEDcNBdmXRwW0QcMpajSCs_bNagtlfsdMur7u3j6gaAur8EALw_wcB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "addinfourl instance has no attribute 'status_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5d8a3f5693e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#soup = BeautifulSoup(page.content, 'html.parser')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: addinfourl instance has no attribute 'status_code'"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import BeautifulSoup\n",
    "newUrl =\"http://jacknorthrup.com/Documentation/index.html\"\n",
    "page = urllib2.urlopen(newUrl)\n",
    "#print (page)\n",
    "#page\n",
    "#soup = BeautifulSoup(page.content, 'html.parser')\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractBasicAuthHandler',\n",
       " 'AbstractDigestAuthHandler',\n",
       " 'AbstractHTTPHandler',\n",
       " 'BaseHandler',\n",
       " 'CacheFTPHandler',\n",
       " 'FTPHandler',\n",
       " 'FileHandler',\n",
       " 'HTTPBasicAuthHandler',\n",
       " 'HTTPCookieProcessor',\n",
       " 'HTTPDefaultErrorHandler',\n",
       " 'HTTPDigestAuthHandler',\n",
       " 'HTTPError',\n",
       " 'HTTPErrorProcessor',\n",
       " 'HTTPHandler',\n",
       " 'HTTPPasswordMgr',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HTTPRedirectHandler',\n",
       " 'HTTPSHandler',\n",
       " 'OpenerDirector',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'Request',\n",
       " 'StringIO',\n",
       " 'URLError',\n",
       " 'UnknownHandler',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__version__',\n",
       " '_cut_port_re',\n",
       " '_have_ssl',\n",
       " '_opener',\n",
       " '_parse_proxy',\n",
       " '_safe_gethostbyname',\n",
       " 'addinfourl',\n",
       " 'base64',\n",
       " 'bisect',\n",
       " 'build_opener',\n",
       " 'ftpwrapper',\n",
       " 'getproxies',\n",
       " 'hashlib',\n",
       " 'httplib',\n",
       " 'install_opener',\n",
       " 'localhost',\n",
       " 'mimetools',\n",
       " 'os',\n",
       " 'parse_http_list',\n",
       " 'parse_keqv_list',\n",
       " 'posixpath',\n",
       " 'proxy_bypass',\n",
       " 'quote',\n",
       " 'random',\n",
       " 'randombytes',\n",
       " 're',\n",
       " 'request_host',\n",
       " 'socket',\n",
       " 'splitattr',\n",
       " 'splithost',\n",
       " 'splitpasswd',\n",
       " 'splitport',\n",
       " 'splittag',\n",
       " 'splittype',\n",
       " 'splituser',\n",
       " 'splitvalue',\n",
       " 'ssl',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'toBytes',\n",
       " 'unquote',\n",
       " 'unwrap',\n",
       " 'url2pathname',\n",
       " 'urlopen',\n",
       " 'urlparse',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib2\n",
    "dir(urllib2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Request in module urllib2:\n",
      "\n",
      "class Request\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |  \n",
      " |  __init__(self, url, data=None, headers={}, origin_req_host=None, unverifiable=False)\n",
      " |  \n",
      " |  add_data(self, data)\n",
      " |  \n",
      " |  add_header(self, key, val)\n",
      " |  \n",
      " |  add_unredirected_header(self, key, val)\n",
      " |  \n",
      " |  get_data(self)\n",
      " |  \n",
      " |  get_full_url(self)\n",
      " |  \n",
      " |  get_header(self, header_name, default=None)\n",
      " |  \n",
      " |  get_host(self)\n",
      " |  \n",
      " |  get_method(self)\n",
      " |  \n",
      " |  get_origin_req_host(self)\n",
      " |  \n",
      " |  get_selector(self)\n",
      " |  \n",
      " |  get_type(self)\n",
      " |  \n",
      " |  has_data(self)\n",
      " |  \n",
      " |  has_header(self, header_name)\n",
      " |  \n",
      " |  has_proxy(self)\n",
      " |  \n",
      " |  header_items(self)\n",
      " |  \n",
      " |  is_unverifiable(self)\n",
      " |  \n",
      " |  set_proxy(self, host, type)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from urllib2 import Request \n",
    "help(Request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully built PyDispatcher Twisted\n",
    "Installing collected packages: PyDispatcher, zope.interface, constantly, incremental, \n",
    "attrs, Automat, hyperlink, Twisted, queuelib, cssselect, w3lib, parsel, pyasn1-modules, \n",
    "service-identity, scrapy\n",
    "Successfully installed Automat-0.6.0 PyDispatcher-2.0.5 Twisted-17.5.0 \n",
    "attrs-17.2.0 constantly-15.1.0 cssselect-1.0.1 hyperlink-17.3.0 incremental-17.5.0 \n",
    "parsel-1.2.0 pyasn1-modules-0.0.11 queuelib-1.4.2 scrapy-1.4.0 service-identity-17.0.0 \n",
    "w3lib-1.18.0 zope.interface-4.4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://jacknorthrup.com/Documentation/index.html\")\n",
    "page\n",
    "\n",
    "print page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "web = \"http://jacknorthrup.com/Documentation/index.html\"\n",
    "page = urllib2.urlopen(web)\n",
    "soup = BeautifulSoup(page, \"html.parser\", from_encoding=\"gb18030\")\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from urllib2 import Request \n",
    "\n",
    "html = urllib2.urlopen(\"http://jacknorthrup.com\")\n",
    "print(html.read())\n",
    "#print (html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-908c45f11076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mname_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h1\"\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprice_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"price\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#import libraries \n",
    "import urllib2  # urllib2 is used to fetch url(s) via urlopen()\n",
    "from bs4 import BeautifulSoup   # when importing Beautiful Soup dont add 4.   \n",
    "from datetime import datetime  # contains functions and classes for working with dates and times, separately and together\n",
    "\n",
    "newUrl =\"http://jacknorthrup.com/Documentation/\"\n",
    "page = urllib2.urlopen(newUrl)\n",
    "\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "name_store = soup.find(\"h1\" ,  attrs={\"class\": \"name\"} )\n",
    "data_name = name_store.text.strip()\n",
    "price_store = soup.find(\"div\", attrs={\"class\": \"price\"})\n",
    "\n",
    "price = price_store.text\n",
    "print  data_name\n",
    "print price\n",
    "t2 = datetime() \n",
    "total = t2 - t1 \n",
    "print  \"scraping completed in \", total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named zope.interface",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1f898168d3fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mItem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSpider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHtmlXPathSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/scrapy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Apply monkey patches to fix issues in external libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/scrapy/_monkeypatches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Undo what Twisted's perspective broker adds to pickle register\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# to prevent bugs like Twisted#7989 while serializing requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyles\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Remove only entries with twisted serializers for non-twisted types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopyreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/twisted/persisted/styles.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Twisted Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/twisted/python/log.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mzope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PY3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named zope.interface"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.spider import BaseSpider\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "import zope.interface\n",
    "\n",
    "class BillBoardItem(Item):\n",
    "    date = Field()\n",
    "    song = Field()\n",
    "    artist = Field()\n",
    "\n",
    "\n",
    "BASE_URL = \"http://www.billboard.com/charts/%s/hot-100\"\n",
    "\n",
    "\n",
    "class BillBoardSpider(BaseSpider):\n",
    "    name = \"billboard_spider\"\n",
    "    allowed_domains = [\"billboard.com\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        date = datetime.date(year=1958, month=8, day=9)\n",
    "\n",
    "        self.start_urls = []\n",
    "        while True:\n",
    "            if date.year >= 2013:\n",
    "                break\n",
    "\n",
    "            self.start_urls.append(BASE_URL % date.strftime('%Y-%m-%d'))\n",
    "            date += datetime.timedelta(days=7)\n",
    "\n",
    "    def parse(self, response):\n",
    "        hxs = HtmlXPathSelector(response)\n",
    "        date = hxs.select('//span[@class=\"chart_date\"]/text()').extract()[0]\n",
    "\n",
    "        songs = hxs.select('//div[@class=\"listing chart_listing\"]/article')\n",
    "        for song in songs:\n",
    "            item = BillBoardItem()\n",
    "            item['date'] = date\n",
    "            try:\n",
    "                item['song'] = song.select('.//header/h1/text()').extract()[0]\n",
    "                item['artist'] = song.select('.//header/p[@class=\"chart_info\"]/a/text()').extract()[0]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named zope.interface",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-100eb3ef7a26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#help(scrapy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/scrapy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Apply monkey patches to fix issues in external libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/scrapy/_monkeypatches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Undo what Twisted's perspective broker adds to pickle register\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# to prevent bugs like Twisted#7989 while serializing requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyles\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Remove only entries with twisted serializers for non-twisted types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopyreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/twisted/persisted/styles.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Twisted Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jack/anaconda2/lib/python2.7/site-packages/twisted/python/log.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mzope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PY3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named zope.interface"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "#help(scrapy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DefaultPY",
   "language": "python",
   "name": "default_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
